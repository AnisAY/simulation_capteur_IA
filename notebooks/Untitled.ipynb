{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4990c093-8300-4027-bf96-1743e864a113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.8/site-packages (7.1.14)\n",
      "Requirement already satisfied: delta-spark==2.2.0 in /opt/conda/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: pyspark<3.4.0,>=3.3.0 in /usr/local/spark-3.3.2-bin-hadoop3/python (from delta-spark==2.2.0) (3.3.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from delta-spark==2.2.0) (4.11.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from minio) (2022.9.24)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from minio) (1.26.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=1.0.0->delta-spark==2.2.0) (3.9.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.8/site-packages (from pyspark<3.4.0,>=3.3.0->delta-spark==2.2.0) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install minio delta-spark==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c3f633-54b7-4335-bd13-46962ef8ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import urllib3\n",
    "\n",
    "from minio import Minio\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import minio\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9283e20c-a6c0-42c2-bab6-ead5f52494fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false'),\n",
       " ('spark.driver.host', 'd63a786ad49c'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-core_2.12-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/hadoop-aws-3.3.2.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-storage-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-1.12.367.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/s3-2.18.41.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-bundle-1.11.1026.jar'),\n",
       " ('spark.hadoop.fs.s3a.access.key', 'minio'),\n",
       " ('spark.hadoop.fs.s3a.path.style.access', 'true'),\n",
       " ('spark.app.name', 'pyspark-rdd-demo-2023-05-13 10:53:10.114898'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://d63a786ad49c:39725/jars/aws-java-sdk-1.12.367.jar,spark://d63a786ad49c:39725/jars/aws-java-sdk-bundle-1.11.1026.jar,spark://d63a786ad49c:39725/jars/s3-2.18.41.jar,spark://d63a786ad49c:39725/jars/delta-storage-2.2.0.jar,spark://d63a786ad49c:39725/jars/delta-core_2.12-2.2.0.jar,spark://d63a786ad49c:39725/jars/hadoop-aws-3.3.2.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.driver.port', '39725'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars',\n",
       "  'file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-core_2.12-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/hadoop-aws-3.3.2.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-storage-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-1.12.367.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/s3-2.18.41.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-bundle-1.11.1026.jar'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.app.startTime', '1683975192156'),\n",
       " ('spark.app.submitTime', '1683975192016'),\n",
       " ('spark.hadoop.fs.s3a.secret.key', 'minio123'),\n",
       " ('spark.app.id', 'app-20230513105312-0000'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"pyspark-rdd-demo-{}\".format(datetime.today()))\n",
    "        .master(\"spark://spark-master:7077\")      \n",
    "        .getOrCreate())\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b6b2e8-e229-47fd-84d7-6174d32a8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = spark.read.csv(\"s3a://donnes-capteurs/donnes_capteurs_13-05-23.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2834aa95-2a9d-4b87-931b-ecf26365be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>entrance_amount</th>\n",
       "      <th>exit_amount</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>parking_entrance</th>\n",
       "      <th>parking_exit</th>\n",
       "      <th>parking_actual_vehicle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>20.978106526032036</td>\n",
       "      <td>47.53296947848155</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>19.75909362330829</td>\n",
       "      <td>37.612915439352506</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>24.084290631587795</td>\n",
       "      <td>52.65643133784484</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>24.02563620852396</td>\n",
       "      <td>48.24337238214185</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>23.304928413054203</td>\n",
       "      <td>66.58388432907365</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>16.459926354265733</td>\n",
       "      <td>62.705480007420746</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "      <td>27.357699370753625</td>\n",
       "      <td>61.29498554714034</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>21.147753720623495</td>\n",
       "      <td>48.17493726001879</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>12.387284594709788</td>\n",
       "      <td>47.2316581689888</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>24.504403457766717</td>\n",
       "      <td>55.177506025883275</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>26.125524790070312</td>\n",
       "      <td>60.48045533413692</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>23.021622053364197</td>\n",
       "      <td>62.98231849449339</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>24.983171579318203</td>\n",
       "      <td>45.9235794062286</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>25.46477131711083</td>\n",
       "      <td>64.16992892703783</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>23.716217325587237</td>\n",
       "      <td>52.35145487833061</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>21.43892344187699</td>\n",
       "      <td>54.322296438838244</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>18.45322770454501</td>\n",
       "      <td>55.63835874428173</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>17.458232906236155</td>\n",
       "      <td>66.9475442193477</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>27.52577048190547</td>\n",
       "      <td>44.83585025295425</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-05-13 11:12:49.597652</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>27.03699383886332</td>\n",
       "      <td>59.30082620133577</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp entrance_amount exit_amount  \\\n",
       "0   2023-05-13 11:12:49.597652              13          41   \n",
       "1   2023-05-13 11:12:49.597652              33           9   \n",
       "2   2023-05-13 11:12:49.597652              41          35   \n",
       "3   2023-05-13 11:12:49.597652               9          27   \n",
       "4   2023-05-13 11:12:49.597652              28           2   \n",
       "5   2023-05-13 11:12:49.597652               6          33   \n",
       "6   2023-05-13 11:12:49.597652              44          17   \n",
       "7   2023-05-13 11:12:49.597652              38          36   \n",
       "8   2023-05-13 11:12:49.597652              31           6   \n",
       "9   2023-05-13 11:12:49.597652              36          32   \n",
       "10  2023-05-13 11:12:49.597652              47          19   \n",
       "11  2023-05-13 11:12:49.597652               3          23   \n",
       "12  2023-05-13 11:12:49.597652              13          16   \n",
       "13  2023-05-13 11:12:49.597652              29          44   \n",
       "14  2023-05-13 11:12:49.597652              15          43   \n",
       "15  2023-05-13 11:12:49.597652               7          13   \n",
       "16  2023-05-13 11:12:49.597652              15          29   \n",
       "17  2023-05-13 11:12:49.597652               7          44   \n",
       "18  2023-05-13 11:12:49.597652               9          30   \n",
       "19  2023-05-13 11:12:49.597652              26          18   \n",
       "\n",
       "           temperature            humidity parking_entrance parking_exit  \\\n",
       "0   20.978106526032036   47.53296947848155                2            1   \n",
       "1    19.75909362330829  37.612915439352506                2            2   \n",
       "2   24.084290631587795   52.65643133784484                1            2   \n",
       "3    24.02563620852396   48.24337238214185                2            2   \n",
       "4   23.304928413054203   66.58388432907365                3            2   \n",
       "5   16.459926354265733  62.705480007420746                2            1   \n",
       "6   27.357699370753625   61.29498554714034                1            4   \n",
       "7   21.147753720623495   48.17493726001879                2            2   \n",
       "8   12.387284594709788    47.2316581689888                2            1   \n",
       "9   24.504403457766717  55.177506025883275                3            2   \n",
       "10  26.125524790070312   60.48045533413692                3            3   \n",
       "11  23.021622053364197   62.98231849449339                1            3   \n",
       "12  24.983171579318203    45.9235794062286                1            2   \n",
       "13   25.46477131711083   64.16992892703783                2            2   \n",
       "14  23.716217325587237   52.35145487833061                1            4   \n",
       "15   21.43892344187699  54.322296438838244                4            2   \n",
       "16   18.45322770454501   55.63835874428173                3            2   \n",
       "17  17.458232906236155    66.9475442193477                1            2   \n",
       "18   27.52577048190547   44.83585025295425                2            3   \n",
       "19   27.03699383886332   59.30082620133577                2            4   \n",
       "\n",
       "   parking_actual_vehicle  \n",
       "0                     390  \n",
       "1                     264  \n",
       "2                     192  \n",
       "3                      96  \n",
       "4                     473  \n",
       "5                     165  \n",
       "6                     214  \n",
       "7                     348  \n",
       "8                     396  \n",
       "9                     293  \n",
       "10                    496  \n",
       "11                    466  \n",
       "12                     66  \n",
       "13                    212  \n",
       "14                     78  \n",
       "15                    420  \n",
       "16                    160  \n",
       "17                     89  \n",
       "18                    425  \n",
       "19                    137  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ecd42c-4176-4715-aef8-a081c4a051fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(3000).write.format(\"delta\").save(\"s3a://donnes-capteurs/testfile.txt\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2900c0de-3da3-4c3c-8f87-1a8baa3ddb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "fdd = sc.textFile(\"s3a://donnes-capteurs/testfile.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fed125-5091-4743-a1f9-ebfc55f69019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(iter, features_cols, labelCol):\n",
    "    # create LinearRegression estimator and set parameters\n",
    "    lr = LinearRegression(maxIter=iter,\n",
    "                          featuresCol=features_cols,\n",
    "                          labelCol=labelCol)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def set_train_and_validation_ds(data, seed):\n",
    "    return data.randomSplit([0.7, 0.3], seed=seed)\n",
    "\n",
    "\n",
    "def test_model(spark, model):\n",
    "    # Methode pour tester le modèle avec un jeu de données test\n",
    "    testData = spark.read.format(\"libsvm\").load(\"s3a://donnes-capteurs/testfile.txt\")\n",
    "    predictions = model.transform(testData)\n",
    "    predictions.show()\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    # Methode pour sauvegarder le modèle vers un lieu spécifié\n",
    "    model.save(\"saved_model/model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11290c71-9113-40cb-91e4-86528249044e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:503\u001B[0m, in \u001B[0;36mParams._set\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 503\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypeConverter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:236\u001B[0m, in \u001B[0;36mTypeConverters.toString\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not convert \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m to string type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(value))\n",
      "\u001B[0;31mTypeError\u001B[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m columns_necessaire \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentrance_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparking_entrance\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparking_exit\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      2\u001B[0m target \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparking_actual_vehicle\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns_necessaire\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [9], line 3\u001B[0m, in \u001B[0;36mcreate_model\u001B[0;34m(iter, features_cols, labelCol)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_model\u001B[39m(\u001B[38;5;28miter\u001B[39m, features_cols, labelCol):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# create LinearRegression estimator and set parameters\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     lr \u001B[38;5;241m=\u001B[39m \u001B[43mLinearRegression\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaxIter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mfeaturesCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mlabelCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabelCol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lr\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/__init__.py:135\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/regression.py:332\u001B[0m, in \u001B[0;36mLinearRegression.__init__\u001B[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid\n\u001B[1;32m    330\u001B[0m )\n\u001B[1;32m    331\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetParams\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/__init__.py:135\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/regression.py:363\u001B[0m, in \u001B[0;36mLinearRegression.setParams\u001B[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001B[39;00m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03mSets params for linear regression.\u001B[39;00m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    362\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:505\u001B[0m, in \u001B[0;36mParams._set\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    503\u001B[0m             value \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mtypeConverter(value)\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 505\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid param value given for param \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (p\u001B[38;5;241m.\u001B[39mname, e))\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paramMap[p] \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "columns_necessaire = [\"entrance_amount\", \"exit_amount\",\"parking_entrance\", \"parking_exit\"]\n",
    "target = [\"parking_actual_vehicle\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2384b6-f2ca-4a67-ada8-5621d3303577",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'pyspark.sql.dataframe.DataFrame'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:503\u001B[0m, in \u001B[0;36mParams._set\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 503\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypeConverter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:236\u001B[0m, in \u001B[0;36mTypeConverters.toString\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not convert \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m to string type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(value))\n",
      "\u001B[0;31mTypeError\u001B[0m: Could not convert <class 'pyspark.sql.dataframe.DataFrame'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m df_orders_2 \u001B[38;5;241m=\u001B[39m df_orders[columns_necessaire]\n\u001B[1;32m      2\u001B[0m df_target \u001B[38;5;241m=\u001B[39m df_orders[target]\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_orders_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_target\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [9], line 3\u001B[0m, in \u001B[0;36mcreate_model\u001B[0;34m(iter, features_cols, labelCol)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_model\u001B[39m(\u001B[38;5;28miter\u001B[39m, features_cols, labelCol):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# create LinearRegression estimator and set parameters\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     lr \u001B[38;5;241m=\u001B[39m \u001B[43mLinearRegression\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaxIter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mfeaturesCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mlabelCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabelCol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lr\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/__init__.py:135\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/regression.py:332\u001B[0m, in \u001B[0;36mLinearRegression.__init__\u001B[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid\n\u001B[1;32m    330\u001B[0m )\n\u001B[1;32m    331\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetParams\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/__init__.py:135\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/regression.py:363\u001B[0m, in \u001B[0;36mLinearRegression.setParams\u001B[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001B[39;00m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03mSets params for linear regression.\u001B[39;00m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    362\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:505\u001B[0m, in \u001B[0;36mParams._set\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    503\u001B[0m             value \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mtypeConverter(value)\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 505\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid param value given for param \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (p\u001B[38;5;241m.\u001B[39mname, e))\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paramMap[p] \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'pyspark.sql.dataframe.DataFrame'> to string type"
     ]
    }
   ],
   "source": [
    "df_orders_2 = df_orders[columns_necessaire]\n",
    "df_target = df_orders[target]\n",
    "create_model(10, df_orders_2, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee7806-daf2-4629-8539-ad0684272171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}